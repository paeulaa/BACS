---
title: "HW2"
author: '107070008'
output:
  pdf_document: default
  html_document:
    df_print: paged
---
**Question 1)**  

(a) Create and visualize a new “Distribution 2”: a combined dataset (n=800) that is negatively skewed (tail stretches to the left). Change the mean and standard deviation of d1, d2, and d3 to achieve this new distribution. Compute the mean and median, and draw lines showing the mean (thick line) and median (thin line).

```{r 1a}
d1 <- rnorm(n=500, mean=45, sd=5)
d2 <- rnorm(n=200, mean=30, sd=5)
d3 <- rnorm(n=100, mean=15, sd=5)
d123 <- c(d1, d2, d3)
plot(density(d123), col="blue", lwd=2, main = "Distribution 2")
abline(v=mean(d123), lwd = 2)
abline(v=median(d123))
mean(d123)
median(d123)
```

(b) Create a “Distribution 3”: a single dataset that is normally distributed (bell-shaped, symmetric) -- you do not need to combine datasets, just use the rnorm() function to create a single large dataset (n=800). Show your code, compute the mean and median, and draw lines showing the mean (thick line) and median (thin line).

```{r 1b}
d1_ <- rnorm(n=800, mean=30, sd=5)
plot(density(d1_), col="blue", lwd=2, main = "Distribution 3")
abline(v=mean(d1_), lwd = 2)
abline(v=median(d1_), col = "green")
mean(d1_)
median(d1_)
```

(c) In general, which measure of central tendency (mean or median) do you think will be more sensitive (will change more) to outliers being added to your data?
>Ans:Means are more sensitive,because they are more susceptible to extreme values than medians.

**Question 2)**

a) Create a random dataset (call it ‘rdata’) that is normally distributed with: n=2000, mean=0, sd=1.  Draw a density plot and put a solid vertical line on the mean, and dashed vertical lines at the 1st, 2nd, and 3rd standard deviations to the left and right of the mean. You should have a total of 7 vertical lines (one solid, six dashed).

```{r 2a}
rdata <- rnorm(n=2000, mean=0, sd=1)
plot(density(rdata), col="blue", lwd=2, main = "random dataset")
abline(v=mean(rdata), lwd = 2)
abline(v=mean(rdata)-sd(rdata), lty="dashed")
abline(v=mean(rdata)+sd(rdata), lty="dashed")
abline(v=mean(rdata)-2*sd(rdata), lty="dashed")
abline(v=mean(rdata)+2*sd(rdata), lty="dashed")
abline(v=mean(rdata)-3*sd(rdata), lty="dashed")
abline(v=mean(rdata)+3*sd(rdata), lty="dashed")
```

b) Using the quantile() function, which data points correspond to the 1st, 2nd, and 3rd quartiles (i.e., 25th, 50th, 75th percentiles) of rdata? How many standard deviations away from the mean (divide by standard-deviation; keep positive or negative sign) are those points corresponding to the 1st, 2nd, and 3rd quartiles?

```{r 2b}
quartiles = quantile(rdata)
#(1)
quartiles[2:4]
#(2)
(quartiles[2:4]-mean(rdata))/sd(rdata)
```

c) Now create a new random dataset that is normally distributed with: n=2000, mean=35, sd=3.5. 
In this distribution, how many standard deviations away from the mean (use positive or negative) are those points corresponding to the 1st and 3rd quartiles? Compare your answer to (b)

```{r 2c}
new_rdata <- rnorm(n=2000, mean=35, sd=3.5)
new_quartiles = quantile(new_rdata)
(new_quartiles[2]-mean(new_rdata))/sd(new_rdata)
(new_quartiles[4]-mean(new_rdata))/sd(new_rdata)
```
>Ans:Compared to (b), the amount of standard deviation away from the mean in (c) is almost the same with (b)'s. 

d) Finally, recall the dataset d123 shown in the description of question 1. In that distribution, how many standard deviations away from the mean (use positive or negative) are those data points corresponding to the 1st and 3rd quartiles? Compare your answer to (b)

```{r 2d}
d1 <- rnorm(n=500, mean=15, sd=5)
d2 <- rnorm(n=200, mean=30, sd=5)
d3 <- rnorm(n=100, mean=45, sd=5)
d123 <- c(d1, d2, d3)
d123_quartiles = quantile(d123)
(d123_quartiles[2]-mean(d123))/sd(d123)
(d123_quartiles[4]-mean(d123))/sd(d123)
```
>Ans:The result are the similar with answer (c). Compared to (b), the amount of standard deviation away from the mean in (d) is almost the same with (b)'s.

**Question 3)**

a) From the question on the forum, which formula does Rob Hyndman’s answer (1st answer) suggest to use for bin widths/number? Also, what does the Wikipedia article say is the benefit of that formula?

>Ans: h=2*IQR(x)/n^(1/3) which is based on the interquartile range, denoted by IQR. It replaces 3.5sd() of Scott's rule with 2 IQR, which is less sensitive than the standard deviation to outliers in data.

b) Compute the bin widths (h) and number of bins (k) according to each of the following formula:
i. Sturges’ formula
ii. Scott’s normal reference rule (uses standard deviation)
iii. Freedman-Diaconis’ choice (uses IQR)

```{r 3b}
rand_data <- rnorm(800, mean=20, sd = 5) 
#Sturges' formula
k_1 <- ceiling(log2(length(rand_data))+1)
h_1 <- (max(rand_data) - min(rand_data))/k_1
k_1
h_1
#Scott’s normal reference rule
h_2 <- 3.49*sd(rand_data)/(length(rand_data)^(1/3))
k_2 <- ceiling((max(rand_data) - min(rand_data))/h_2)
k_2
h_2
#Freedman-Diaconis’ choice
h_3 <- 2*IQR(rand_data)/(length(rand_data)^(1/3))
k_3 <- ceiling((max(rand_data) - min(rand_data))/h_3)
k_3
h_3
```

c) From your answers above, in which of the three methods does the bin width (h) change the least when outliers are added (i.e., which is least sensitive to outliers), and (briefly) WHY do you think that is?

```{r 3c}
out_data <- c(rand_data, runif(10, min=40, max=60))
#(a)
k1 <- ceiling(log2(length(out_data))+1)
h1 <- (max(out_data) - min(out_data))/k1
#(b)
h2 <- 3.49*sd(out_data)/(length(out_data)^(1/3))
#(c)
h3 <- 2*IQR(out_data)/(length(out_data)^(1/3))
c(h_1, h_2, h_3)
c(h1, h2, h3)
```
>Ans:Freedman-Diaconis’ choice, because it is based on interquartile range, and make the result less sensitive to outer data than simple standard deviation. 

